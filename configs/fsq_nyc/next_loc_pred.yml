misc:
  dataset: fsq_nyc
  # 3816 known locations, ranging from 2 to 3897
  total_loc_num: 3818

  # 535 users, ranging from 1 to 535
  total_user_num: 536

  # Embedding - hparams for MHSA model
  if_embed_user: True
  if_embed_poi: False
  if_embed_time: True
  if_embed_duration: False
  if_embed_spatial: False
  if_embed_roadnet: False
  loc_embed_method: skipgram # vanilla, ctle, poi2vec, skipgram, teaser, etc.
  
  loc_enc_ckpt_path: pretrained/fsq_nyc_skipgram_inductive10pct_20250112_203545  # None if loc_embed_method is vanilla
  loc_type: lon-lat  # lon-lat or x-y
  loc_is_static: True

  previous_day: 7  # how many days in the past we consider
  verbose: True
  debug: False
  batch_size: 128
  print_step: 20
  num_workers: 0
  day_selection: default
  inductive_pct: 10  # 0 for conventional setting, 10 for inductive setting

embedding:
  base_emb_size: 128

model:
  networkName: mhsa
  # only for transformer-based architecture
  num_encoder_layers: 6
  nhead: 8
  dim_feedforward: 256
  fc_dropout: 0.1

optimiser:
  optimizer: Adam
  max_epoch: 100
  lr: 0.001
  weight_decay: 0.000001
  # lr: 0.01
  # for Adam
  beta1: 0.9
  beta2: 0.999
  # for SGD
  momentum: 0.98
  # for warmup
  num_warmup_epochs: 2
  num_training_epochs: 50
  # for decay
  patience: 3
  lr_step_size: 1
  lr_gamma: 0.1

dataset:
  source_root: ./data/
  save_root: ./outputs_conventional
  inductive_save_root: ./outputs_inductive

training:
  device: cuda:0
